\section{Особенности вычислительной архитектуры CUDA}

Архитектура CUDA представляет собой набор программных и аппаратных модулей, позволяющих получить значительный прирост производительности при решении многих задач благодаря переносу больших объемов вычислений на графический ускоритель. Программная часть комплекса состоит из набора библиотек, драйвера, а так же SDK, содержащего специализированны компилятор. Аппаратная часть является платой расширения, содержащей 16 мультипроцессоров. Каждый мультипроцессор имеет SIMD архитектуру и способен параллельно исполнять 8 одинаковых инструкций над различными наборами данных, однако с точки зрения разработчика все выглядит так, как будто каждый мультипроцессор параллельно выполняет 32 потока. 

Каждый мультипроцессор имеет небольшое количество быстродействующей памяти на борту, а плате содержится основная оперативная память. Подробнее особенности устройства мультипроцессоров и их взаимодействия с памятью рассмотрены разделе~\ref{c:mem_model}

При программировании при помощи CUDA, GPU рассматривается, как вычислительное устройство, способное параллельно выполнять очень большое число потоков. Ускоритель используется в качестве сопроцессора, на который CPU может передать трудоемкие вычисления, хорошо распараллеливаемые по данным (data-parallel).

Другими словами, функция, которая многократно применяется к независимым порциям данных, может быть выполнена на ускорителе во множестве параллельных потоков. Для этого ее необходимо скомпилирована в набор команд, поддерживаемых вычислительным акселератором, и полученный код, называемый ядром (kernel), должен быть передан на ускоритель.

Центральный процессор и ускоритель работают с различными модулями памяти, называемыми основной памятью (host memory) и памятью устройства (device memory). Для передачи данных между этими видами памяти используется специальное API.

\subsection{Иерархия потоков}
Потоки, выполняющиеся на ускорителе организованы в решетку (grid) блоков (blocks), показанную на рис.~\ref{fig:cuda_grid}.

\begin{figure}[ht]
\center
\includegraphics[width=100mm]{cuda_grid}
\caption{Иерархия потоков}
\label{fig:cuda_grid}
\end{figure}

Блоком потоков называется группа, способная обмениваться данными при помощи быстрой разделяемой памяти (shared memory), а также синхронизировать свое выполнение для координации работы с памятью. Синхронизация производится благодаря указанию в коде ядра специальных точек синхронизации, где выполнение потоков приостанавливается до тех пор, пока все потоки блока не достигнут этой точки. Все потоки одного блока физически выполняются на одном мультипроцессоре, однако каждый мультипроцессоров может выполнять несколько потоков.

Каждый поток имеет собственный идентификатор (thread ID), являющийся номером потока в блоке. Чтобы упростить обработку данных, имеющих многомерную структуру (изображения или объемные массивы данных) имеется возможность задания блока, как двух или трехмерного массива указанного размера. В общем случае потоки обозначаются трехмерными векторами $(x, y, z)$. Идентификатор потока вычисляется по формуле $threadId = x + yD_x + zD_xD_y$, где $D_x$, $D_y$ и $D_z$ обозначают соответствующие размерности блока. Размеры блока задаются в момент запуска ядра и являются одинаковыми для всех блоков решетки.
 
Существует ограничение на количество потоков в одном блоке. Для устройства, использованного при написании данной работы оно равняется 512. Однако блоки одинакового размера, выполняющие одинаковое ядро могут быть объединены в решетку. Таким образом, при вызове ядра можно создать значительно больше потоков. Количество блоков, которое может параллельно обрабатываться акселератором зависит от характеристик конкретного устройства. Если количество блоков превышает параллельные возможности ускорителя, он последовательно начинает обрабатывать новые блоки по мере освобождения вычислительных ресурсов. Подробнее механизм распределения заданий исследуется в разделе \ref{c:cuda_hw}

Так как потоки разных блоков могут выполняться на разных мультипроцессорах, обмен данными между блоками затруднен. Единственным стандартным методом синхронизации потоков из различных блоков, который поддерживался ускорителем, использованным при написании данной работы, было ожидание завершения выполнения всего ядра. Стоит заметить, что в более новых ускорителях реализована поддержка атомарных функций, но здесь они не рассматриваются.

Каждый блок имеет свой идентификатор (block ID). Блоки могут быть организованы в двухмерную решетку указанного размера. В этом случае, блоки обозначаются двумерными векторами $(x, y)$. Идентификатор блока в этом случае вычисляется по формуле $blockId = x + yD_x$, где $D_x$ --- количество столбцов в блоке.

\subsection{Модель памяти}
\label{c:mem_model}
Модель памяти ускорителя изображена на рис.~\ref{fig:cuda_memory}.

\begin{figure}[ht]
\center
\includegraphics[width=100mm]{cuda_memory}
\caption{Модель памяти}
\label{fig:cuda_memory}
\end{figure}

Регистры и локальная память доступны в пределах одного потока, разделяемая память позволяет обмениваться данными потокам, находящимся в одном блоке, а глобальная, константная и память текстур являются общими для всех потоков. При этом константная память и память текстур доступны только для чтения, а остальные виды --- для чтения и записи.

Глобальная, константная и текстурная памяти оптимизированы для различных режимов использования. Дополнительно, текстурная память поддерживает различные режимы адресации и методы фильтрации данных, применяемые в трехмерной графике для работы с текстурами.

При написании вычислительных ядер следует иметь ввиду, опреции с глобальной памятью требуют достаточно много времени, так как доступ к ней не кэшируется. Одним из способов борьбы с этими задержками является способ управление потоками, описанный в разделе \ref{c:cuda_hw} Другим способом является использование быстрой распределяемой памяти в качестве кэша. Другие методы оптимизации описаны в \cite{cuda_struct_alg} и \cite{cuda_optim}.

\subsection{Аппаратная архитектура ускорителя}
\label{c:cuda_hw}

Для написания эффективных програм для CUDA необходимо знать особенности аппаратного устройства ускорителя. Его структура изображена на рис.~\ref{fig:cuda_hw}.

\begin{figure}[ht]
\center
\includegraphics[width=100mm]{cuda_hw}
\caption{Аппаратная структура ускорителя}
\label{fig:cuda_hw}
\end{figure}

Акселератор реализован как набор мультипроцессоров, имеющих SIMD архитектуру. Каждый такт, все процессоры мультипроцессора, выполняют одну и туже инструкцию, но над разными наборами данных. Каждый мультипроцессор содержит четыре вида памяти:

\begin{itemize}
\item Набор 32-х битных регистров;

\item распределенная память;

\item константный кэш, используемый для ускорения доступа к константной памяти, реализованной ввиде региона памяти устройства, доступного только для чтения;

\item текстурный кэш, используемый для ускорения запросов к текстурам, которые хранятся в участке памяти устройства, доступном только для чтения.
\end{itemize}

Глобальное и локальное пространства памяти реализованы как доступные для чтения и записи регионы памяти устройства.

\subsection{Управление потоками}

Группы блоклов (batch) потоков назначаются на исполнение планировщиком заданий. Каждый мультипроцессор последовательно обрабатывает одну группу за другой. Каждый блок обрабатывается только одним мультипроцессором. Благодаря этому разделяемая память размещается в быстродействующей памяти мультипроцессора и работает значительно быстрее глобальной.

Максимальное количество блоков в группе, которую может обработать мультипроцессор, зависит от того, как много регистров использует каждый поток и сколько разделяемой памяти требуется каждому блоку, так как эти ресурсы распределяются между всеми потоками всех блоков, работающих на мультипроцессоре. Если ресурсов недостаточно даже для запуска единственного блока, ядро не может быть запущено.

Блоки, принадлежащие группе, обрабатываемой на мультипроцессоре, называют активными блоками. Потоки активных блоков называются активными потоками. Каждый активный блок разбивается на группы потоков, называемые warp'ами. Каждый warp содержит одно и тоже количество потоков, называемое размером warp'а. Потоки одного warp'а работают в SIMD режиме. Все инструкции этих потоков выполняются синхронно. Если в ядре есть ветвления, и в warp'е есть потоки, идущие в разные ветки, выполнение этих потоков сериализуется. По этой причине расходящиеся ветвления могут снизить производительность.

Warp'ы активных потоков поочередно выполняются мультипроцессором. Ими управляет планировщик потоков, который периодически переключает потоки, минимизируя использование вычислительных ресурсов. Например, когда один warp ожидает данные из памяти или заблокирован, ожидая другие потоки своего блока, потоки других блоков, выполняющихся на данном мультипроцессоре, могут производить математические операции. Это позволяет более рационально использовать вычислительные ресурсы мультипроцессора[].

Количество потоков, арифметические операции которых ускоритель может выполнять параллельно, равняется числу мультипроцессоров умноженному на размер warp'а. Максимальное количество активных потоков равняется кличеству warp'ов на мультипроцессор, умноженному на размер warp'а, умноженному на количество мультипроцессоров. 

На практике такое количество активных потоков достижимо только на достаточно простых ядрах. Требования ядра к регистрам, разделяемой памяти, и размер блока могут ограничить число активных потоков. Отношение реального числа активных потоков к максимально достижимому называется occupancy. Для расчета этой величины по параметрам ядра в CUDA SDK присутствует специальная утилита.

Следует заметить, что для достижения максимальной производительности, при разработке ядер следует знать о большом количестве тонкостей работы ускорителя.

\subsection{Характеристики исследуемой платформы}
При выполнении данной работы тестирование реализации производилось на двух разных ускорителях:
\begin{itemize}
\item NVIDIA GeForce 8800 GTS
\item NVIDIA GeForce GTX 275
\end{itemize}

 Карта NVIDIA GeForce 8800 GTS обладает следующими характеристиками:
 \begin{itemize}
\item 640 мб основной памяти,
\item 14 мультипроцессоров,
\item 32 потока на warp,
\item максимум 24 warp'а на мультипроцессор,
\item максимум 8 блоков на мультипроцессор,
\item 8192 32-х битных регистров на мультипроцессор,
\item 16384 байта разделяемой памяти на мультипроцессор,
\item тактовая частота потоковых процессоров 1.2 Ghz,
\item максимум $14 \cdot 24\cdot 32 = 10752$ активных потоков.
 \end{itemize}

Вторая карта, NVIDIA GeForce GTX 275 обладает следующими характеристиками:
 \begin{itemize}
\item 896 мб основной памяти,
\item 30 мультипроцессоров,
\item 32 потока на warp,
\item максимум 32 warp'а на мультипроцессор,
\item максимум 8 блоков на мультипроцессор,
\item 8192 32-х битных регистров на мультипроцессор,
\item 16384 байта разделяемой памяти на мультипроцессор,
\item тактовая частота потоковых процессоров 1.4 Ghz.
\item максимум $30 \cdot 32\cdot 32 = 30720$ активных потоков.
 \end{itemize}
